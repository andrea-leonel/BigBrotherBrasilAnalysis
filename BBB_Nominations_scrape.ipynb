{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecf8e149-3f83-40d0-9c0f-8e9090826147",
   "metadata": {},
   "source": [
    "## Project: An historical analysis of the Big Brother Brasil show\r\n",
    "(Portuguese below)\r\n",
    "\r\n",
    "This script is part of a project that gathers historical data about the Brazilian version of the Big Brother reality show and analyses how it has changed over its 25 years of existence in terms of demographics, audience participation, and other factors. This show is the most-watched programm in Brazil and the goal is to make all this data available for other analysts who wish to explore it. At the time of writing, this is the only one-stop-shop source of BBB data online.\r\n",
    "\r\n",
    "(Portuguese) Projeto: Uma análise histórica do Big Brother Brasil Esse script faz parte de um projeto que coleta dados históricos sobre o Big Brother Brasil de diferentes fontes e os disponibiliza de forma limpa, normalizada e com as devidas conexões. O objetivo desse projeto é tornar os dados acessíveis para outras pessoas que desejarem analisá-los e, no momento da sua publicação, essaé a única fonte de dados consolidada sobre o Big Brother Brasil."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99728e7-5f80-4b84-94b0-a856b0b41aea",
   "metadata": {},
   "source": [
    "## Script: pulling eviction information from Wikipedia\n",
    "(Portuguese below)\n",
    "\n",
    "This script scrapes the Nominations table from the Wikipedia pages. On Wikipedia, this table is divided into 3 with a shared header. Therefore, the script pulls out the headers, normalise them and then splits this table into three.\n",
    "\n",
    "Script: extraindo dados dos paredões da Wikipedia\n",
    "Este script extrai a tabela de Paredões das páginas da Wikipedia. Na Wikipedia, essa tabela é dividida em 3 partes com um header compartilhado. Portanto, o script captura os headers, os normaliza e, em seguida, divide essa tabela em três."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e027f162-505e-4588-a811-0c32fd168bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import display\n",
    "from io import StringIO\n",
    "import csv\n",
    "import gzip\n",
    "from unidecode import unidecode\n",
    "import html5lib\n",
    "import argparse\n",
    "\n",
    "def nominations_scrape(url)\n",
    "\n",
    "# Scraping the Wikipedia page\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html5lib')\n",
    "\n",
    "h2_header = soup.find('h2', {'id': 'Histórico'})\n",
    "desired_table = None\n",
    "\n",
    "if h2_header:\n",
    "    parent_div = h2_header.find_parent('div')\n",
    "    next_div = parent_div.find_next_sibling()\n",
    "    \n",
    "    if next_div:\n",
    "        if next_div.name == \"table\"\n",
    "            desired_table = next_div\n",
    "        else:\n",
    "            desired_table = next_div.find(\"table\", recursive=False)\n",
    "\n",
    "html_to_table = pd.read_html(str(desired_table))\n",
    "Nominations_raw = html_to_table[0]\n",
    "\n",
    "# Normalising headers\n",
    "\n",
    "all_levels = [Nominations_raw.columns.get_level_values(level).tolist() for level in range(Nominations_raw.columns.nlevels)]\n",
    "\n",
    "headers = []\n",
    "\n",
    "for items in zip(*all_levels):\n",
    "    merged_items = []\n",
    "    for i in range(len(items)):\n",
    "        if i == 0 or items[i] != items[i - 1]: \n",
    "            merged_items.append(items[i])\n",
    "        else:\n",
    "            merged_items.append(\"\")\n",
    "    headers.append(\" - \".join(filter(None, merged_items)))\n",
    "\n",
    "headers = [\"\" if \"Unnamed\" in item else item for item in headers]\n",
    "Nominations_raw.columns = headers\n",
    "\n",
    "# Adding the year of the current file\n",
    "\n",
    "Nominations_raw['Edicao'] = url.rsplit('_', 1)[-1]\n",
    "\n",
    "# Splitting the 3 tables\n",
    "\n",
    "Nominations_raw.replace(r'^\\s*$', None, regex=True, inplace=True)\n",
    "\n",
    "divider_index = Nominations_raw[Nominations_raw.isnull().all(axis=1)].index\n",
    "\n",
    "df_part1 = Nominations_raw.iloc[:divider_index[0]] \n",
    "df_part1.columns = headers\n",
    "df_part2 = Nominations_raw.iloc[divider_index[0]+1:divider_index[1]]\n",
    "df_part2.columns = headers\n",
    "df_part3 = Nominations_raw.iloc[divider_index[1]+1:]\n",
    "df_part3.columns = headers\n",
    "\n",
    "Nominations = pd.DataFrame(df_part1)\n",
    "Individual_nominations = pd.DataFrame(df_part2)\n",
    "Eviction_results = pd.DataFrame(df_part3)\n",
    "\n",
    "# Light manipulations to the 3 tables\n",
    "\n",
    "Nominations = Nominations.dropna(axis=1, how='all')\n",
    "\n",
    "Nominations.set_index(Nominations.columns[0])\n",
    "\n",
    "Nominations = Nominations.loc[:, ~Nominations.T.duplicated()]\n",
    "\n",
    "Nominations = Nominations.T\n",
    "Nominations.columns = Nominations.iloc[0]\n",
    "Nominations = Nominations[1:]\n",
    "\n",
    "Nominations['Edicao'] = url.rsplit('_', 1)[-1]\n",
    "\n",
    "Individual_nominations = Individual_nominations.dropna(axis=1, how='all')\n",
    "\n",
    "Individual_nominations.set_index(Individual_nominations.columns[0])\n",
    "\n",
    "Individual_nominations = Individual_nominations.T\n",
    "Individual_nominations.columns = Individual_nominations.iloc[0]\n",
    "Individual_nominations = Individual_nominations[1:]\n",
    "\n",
    "Individual_nominations['Edicao'] = url.rsplit('_', 1)[-1]\n",
    "\n",
    "Eviction_results = Eviction_results.dropna(axis=1, how='all')\n",
    "\n",
    "Eviction_results.set_index(Eviction_results.columns[0])\n",
    "\n",
    "Eviction_results = Eviction_results.loc[:, ~Eviction_results.T.duplicated()]\n",
    "\n",
    "Eviction_results_t = Eviction_results.T\n",
    "Eviction_results_t.columns = Eviction_results_t.iloc[0]\n",
    "Eviction_results = Eviction_results_t[1:]\n",
    "\n",
    "Eviction_results = Eviction_results.loc[:, ~Eviction_results.T.duplicated()]\n",
    "\n",
    "Eviction_results['Edicao'] = url.rsplit('_', 1)[-1]\n",
    "\n",
    "# Save to csv\n",
    "Nominations.to_csv(f'Nominations_{year}.csv')\n",
    "Individual_nominations.to_csv(f'Individual_nominations_{year}.csv')\n",
    "Eviction_results.to_csv(f'Eviction_results_{year}.csv')\n",
    "\n",
    "return Nominations, Individual_nominations, Eviction_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Process a Wikipedia URL.\")\n",
    "    parser.add_argument(\"url\", type=str, help=\"The URL to process\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Call your function with the provided URL\n",
    "    scrape_and_process(args.url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
